{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35199b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096cbef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c450ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d80a1457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date state  ave_temp  ave_temp_uncertainty declaration_title  \\\n",
      "8797    01-1961    AK   -15.443                 0.434                 0   \n",
      "8798    01-1961    AL     4.301                 0.207                 0   \n",
      "8799    01-1961    AR     2.393                 0.281                 0   \n",
      "8800    01-1961    AZ     5.629                 0.313                 0   \n",
      "8801    01-1961    CA     7.005                 0.432                 0   \n",
      "...         ...   ...       ...                   ...               ...   \n",
      "136388  12-2012    VT    -2.191                 0.343                 0   \n",
      "136389  12-2012    WA     0.290                 0.307                 0   \n",
      "136390  12-2012    WI    -3.367                 0.301                 0   \n",
      "136391  12-2012    WV     4.280                 0.240                 0   \n",
      "136392  12-2012    WY    -5.374                 0.375                 0   \n",
      "\n",
      "       disaster_type month  y_data  \n",
      "8797               0    01     0.0  \n",
      "8798               0    01     0.0  \n",
      "8799               0    01     0.0  \n",
      "8800               0    01     0.0  \n",
      "8801               0    01     0.0  \n",
      "...              ...   ...     ...  \n",
      "136388             0    12     0.0  \n",
      "136389             0    12     0.0  \n",
      "136390             0    12     0.0  \n",
      "136391             0    12     0.0  \n",
      "136392             0    12     0.0  \n",
      "\n",
      "[31716 rows x 8 columns]\n",
      "8797     0.0\n",
      "8824     0.0\n",
      "8825     0.0\n",
      "8826     0.0\n",
      "8827     0.0\n",
      "        ... \n",
      "88749    0.0\n",
      "88725    0.0\n",
      "88726    0.0\n",
      "88727    0.0\n",
      "88728    0.0\n",
      "Name: y_data, Length: 19029, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import gc\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "dis_path = './data/us_disaster_declarations.csv'\n",
    "temp_path = './data/GlobalLandTemperaturesByState.csv'\n",
    "states_path = './data/states.csv'\n",
    "\n",
    "# load list of states\n",
    "states = {}\n",
    "with open(states_path) as f:\n",
    "    next(f)\n",
    "\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        states[l[0].strip()] = l[1].strip()\n",
    "\n",
    "# print(states)\n",
    "\n",
    "# filter disaster dataset\n",
    "dis_data = pd.read_csv(dis_path)[['state', 'declaration_date', 'incident_type','declaration_title']].rename({'declaration_date': 'date'}, axis=1)\n",
    "dis_data['date'] = dis_data['date'].astype('datetime64[ns]').dt.strftime('%m-%Y')\n",
    "dis_data = dis_data.drop_duplicates(subset=['incident_type', 'declaration_title', 'date', 'state'], keep='first')\n",
    "# dis_data = dis_data.groupby(['state', 'date']).count()\n",
    "dis_data['disaster_type'] = dis_data['incident_type']\n",
    "dis_data = dis_data.rename({'incident_type': 'disaster_occurrence'}, axis=1)\n",
    "dis_data['disaster_occurrence'] = np.ones(dis_data['disaster_occurrence'].shape)\n",
    "# dis_data = dis_data.reset_index()\n",
    "\n",
    "# print(dis_data)\n",
    "\n",
    "dis_data.to_csv('./data/test_disasters_state_month.csv', index=False)\n",
    "\n",
    "# filter temperature dataset\n",
    "temp_data = pd.read_csv(temp_path)\n",
    "temp_data = temp_data[temp_data['Country'] == 'United States'].dropna()  # filter by United States, remove NaNs\n",
    "temp_data['date'] = temp_data['dt'].astype('datetime64[ns]').dt.strftime('%m-%Y')  # convert string to date, then convert to year\n",
    "temp_data['state'] = temp_data['State'].apply(lambda x: states[x] if x in states else None)  # preprocess state strings\n",
    "temp_data = temp_data.dropna()\n",
    "temp_data = temp_data.groupby(['date', 'state'])  # group by year then state\n",
    "temp_data = temp_data[['AverageTemperature', 'AverageTemperatureUncertainty']].mean().reset_index()  # take average over groups\n",
    "\n",
    "# print(temp_data)\n",
    "\n",
    "temp_data.to_csv('./data/test_temp_state_month.csv', index=False)\n",
    "\n",
    "# join on `Year` and `State`\n",
    "df = pd.merge(temp_data, dis_data, on=['date', 'state'], how='left').set_index(['date', 'state'], drop=True)\n",
    "df.rename({'AverageTemperature': 'ave_temp', 'AverageTemperatureUncertainty': 'ave_temp_uncertainty'}, axis=1, inplace=True)\n",
    "df = df.fillna(0).reset_index()\n",
    "df['month'] = df['date'].astype('datetime64[ns]').dt.strftime('%m')\n",
    "df['year'] = df['date'].astype('datetime64[ns]').dt.strftime('%Y')\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('./data/test_disasters_temp_state_month.csv', index=False)\n",
    "\n",
    "df['y_data'] = df['disaster_occurrence']\n",
    "df = df.drop(['disaster_occurrence'], axis=1)\n",
    "\n",
    "\n",
    "df = df[df.year > \"1960\"]\n",
    "df = df.drop(['year'], axis=1)\n",
    "\n",
    "print(df)\n",
    "df = df.sort_values(by=['date'])\n",
    "x_data = df.iloc[:,0:-1]\n",
    "y_data = df.iloc[:,-1]\n",
    "\n",
    "df = df.drop(['declaration_title','disaster_type'], axis=1)\n",
    "\n",
    "#x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=.5, shuffle=False)\n",
    "\n",
    "train_rows = int(0.6 * x_data.shape[0])\n",
    "test_rows = x_data.shape[0] - train_rows# - train_rows\n",
    "\n",
    "x_train = df.iloc[:train_rows, 0:-1]\n",
    "y_train = df.iloc[:train_rows, -1]\n",
    "x_test = df.iloc[train_rows:, 0:-1]\n",
    "y_test = df.iloc[train_rows:, -1]\n",
    "    \n",
    "print(y_train)\n",
    "\n",
    "X = x_data.iloc[:,2:]\n",
    "y = y_data\n",
    "\n",
    "logR = LogisticRegression()\n",
    "logR.fit(x_train.iloc[:,2:], y_train)\n",
    "y_predict_test = logR.predict_proba(x_test.iloc[:,2:])\n",
    "y_predict_train = logR.predict_proba(x_train.iloc[:,2:])\n",
    "\n",
    "x_test['Disaster_Prob'] = y_predict_test[:,1:]\n",
    "\n",
    "\n",
    "y_predict_test = y_predict_test[:,1:]\n",
    "test_df = pd.DataFrame(y_predict_test, columns = ['Y_predict'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LogisticRegression_Test_Per = x_test[['date', 'state', 'Disaster_Prob']]\n",
    "#print(LogisticRegression_Test_Per.tail(50))\n",
    "\n",
    "LogisticRegression_Test_Per.to_csv('./data/logRPredictTest.csv', index=False)    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "503e0781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95494471 0.04505529]\n",
      " [0.84495342 0.15504658]\n",
      " [0.82706004 0.17293996]\n",
      " ...\n",
      " [0.907349   0.092651  ]\n",
      " [0.95457925 0.04542075]\n",
      " [0.82788755 0.17211245]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "def split_data(X, y, test_size = 0.2):       \n",
    "    import sklearn as sk    \n",
    "    return sk.model_selection.train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, test_size = 0.2)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB    \n",
    "GNBclf = GaussianNB()\n",
    "classfier = GNBclf.fit(X_train, y_train)\n",
    "\n",
    "def score_and_predict(classifier, X_train, y_train, X_test, y_test):    \n",
    "    y_Train_pred = classifier.predict_proba(X_train)\n",
    "    y_Test_pred = classifier.predict_proba(X_test)\n",
    "    #testing_score = metrics.accuracy_score(y_test, y_Test_pred)\n",
    "    #training_score = metrics.accuracy_score(y_train, y_Train_pred)\n",
    "    return y_Train_pred\n",
    "\n",
    "y_pred = score_and_predict(classfier, X_train, y_train, X_test, y_test)\n",
    "#print('Mean accuracy on training data = {} and testing data = {}'.format(training_score.round(2), testing_score.round(2)))\n",
    "print(y_pred)\n",
    "def print_classification_report(y_test, y_pred):       \n",
    "    from sklearn.metrics import classification_report\n",
    "    return (print(classification_report(y_test,y_pred)))\n",
    "\n",
    "#print_classification_report(y_test, y_pred)\n",
    "\n",
    "def roc_curve(classifier, X_test, y_test):      \n",
    "    from sklearn.metrics import roc_curve\n",
    "    import matplotlib.pyplot as plt       \n",
    "    predict_prob = classifier.predict_proba(X_test)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,predict_prob[:, 1])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')        \n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')   \n",
    "    return (plt.show())\n",
    "\n",
    "#roc_curve(classifier, X_test, y_test)\n",
    "from sklearn.svm import SVC\n",
    "SVMclf = SVC(probability=True) \n",
    "SVMclf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-species",
   "metadata": {},
   "source": [
    "# 1. Setting up a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "with open('data_test.csv', 'r') as file:\n",
    "    df = csv.reader(file)\n",
    "    for row in df:\n",
    "        print(row)\n",
    "        \n",
    "df = pd.read_csv('data_test.csv')\n",
    "        \n",
    "#Create the confusion matrix\n",
    "# Importing the dependancies\n",
    "\n",
    "y_pred = df[df.columns[-1]]\n",
    "# Actual values\n",
    "y_act = df[df.columns[2]]\n",
    "\n",
    "# Printing the confusion matrix\n",
    "# The columns will show the instances predicted for each label,\n",
    "# and the rows will show the actual number of instances for each label.\n",
    "\n",
    "print(y_pred)\n",
    "# Printing the precision and recall, among other metrics\n",
    "\n",
    "print(y_act)\n",
    "#confusion matrix\n",
    "print(metrics.confusion_matrix(y_act, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-winning",
   "metadata": {},
   "source": [
    "# 2. Finding other useful model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other model metrics\n",
    "print(metrics.classification_report(y_act, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-ferry",
   "metadata": {},
   "source": [
    "# 3. Create an ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Computing the ROC curve\n",
    "n_classes = y_act.shape[0]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_act[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_act.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#Plotting the ROC curve\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
