{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06613e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7476ceba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08d067",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "098f0095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date state  ave_temp  ave_temp_uncertainty declaration_title  \\\n",
      "8797    01-1961    AK   -15.443                 0.434                 0   \n",
      "8798    01-1961    AL     4.301                 0.207                 0   \n",
      "8799    01-1961    AR     2.393                 0.281                 0   \n",
      "8800    01-1961    AZ     5.629                 0.313                 0   \n",
      "8801    01-1961    CA     7.005                 0.432                 0   \n",
      "...         ...   ...       ...                   ...               ...   \n",
      "136388  12-2012    VT    -2.191                 0.343                 0   \n",
      "136389  12-2012    WA     0.290                 0.307                 0   \n",
      "136390  12-2012    WI    -3.367                 0.301                 0   \n",
      "136391  12-2012    WV     4.280                 0.240                 0   \n",
      "136392  12-2012    WY    -5.374                 0.375                 0   \n",
      "\n",
      "       disaster_type month  y_data  \n",
      "8797               0    01     0.0  \n",
      "8798               0    01     0.0  \n",
      "8799               0    01     0.0  \n",
      "8800               0    01     0.0  \n",
      "8801               0    01     0.0  \n",
      "...              ...   ...     ...  \n",
      "136388             0    12     0.0  \n",
      "136389             0    12     0.0  \n",
      "136390             0    12     0.0  \n",
      "136391             0    12     0.0  \n",
      "136392             0    12     0.0  \n",
      "\n",
      "[31716 rows x 8 columns]\n",
      "          date state  ave_temp  ave_temp_uncertainty\n",
      "8797   01-1961    AK   -15.443                 0.434\n",
      "8798   01-1961    AL     4.301                 0.207\n",
      "8799   01-1961    AR     2.393                 0.281\n",
      "8800   01-1961    AZ     5.629                 0.313\n",
      "8801   01-1961    CA     7.005                 0.432\n",
      "...        ...   ...       ...                   ...\n",
      "88751  08-1969    ND    22.182                 0.220\n",
      "88752  08-1969    NE    23.708                 0.302\n",
      "88753  08-1969    NH    19.595                 0.291\n",
      "88754  08-1969    NJ    22.797                 0.325\n",
      "88755  08-1969    NM    23.583                 0.296\n",
      "\n",
      "[19029 rows x 4 columns]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-046e88a02bb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[0mlogR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m \u001b[0mlogR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m \u001b[0my_predict_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[0my_predict_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1599\u001b[0m                       \u001b[0mpenalty\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1600\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1601\u001b[1;33m             for class_, warm_start_coef_ in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1039\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1041\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 263\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    938\u001b[0m             n_iter_i = _check_optimize_result(\n\u001b[0;32m    939\u001b[0m                 \u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                 extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n\u001b[0m\u001b[0;32m    941\u001b[0m             \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\sklearn\\utils\\optimize.py\u001b[0m in \u001b[0;36m_check_optimize_result\u001b[1;34m(solver, result, max_iter, extra_warning_msg)\u001b[0m\n\u001b[0;32m    241\u001b[0m                 \u001b[1;34m\"    https://scikit-learn.org/stable/modules/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m                 \u001b[1;34m\"preprocessing.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m             ).format(solver, result.status, result.message.decode(\"latin1\"))\n\u001b[0m\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_warning_msg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0mwarning_msg\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"\\n\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mextra_warning_msg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import gc\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "dis_path = './data/us_disaster_declarations.csv'\n",
    "temp_path = './data/GlobalLandTemperaturesByState.csv'\n",
    "states_path = './data/states.csv'\n",
    "\n",
    "# load list of states\n",
    "states = {}\n",
    "with open(states_path) as f:\n",
    "    next(f)\n",
    "\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        states[l[0].strip()] = l[1].strip()\n",
    "\n",
    "# print(states)\n",
    "\n",
    "# filter disaster dataset\n",
    "dis_data = pd.read_csv(dis_path)[['state', 'declaration_date', 'incident_type','declaration_title']].rename({'declaration_date': 'date'}, axis=1)\n",
    "dis_data['date'] = dis_data['date'].astype('datetime64[ns]').dt.strftime('%m-%Y')\n",
    "dis_data = dis_data.drop_duplicates(subset=['incident_type', 'declaration_title', 'date', 'state'], keep='first')\n",
    "# dis_data = dis_data.groupby(['state', 'date']).count()\n",
    "dis_data['disaster_type'] = dis_data['incident_type']\n",
    "dis_data = dis_data.rename({'incident_type': 'disaster_occurrence'}, axis=1)\n",
    "dis_data['disaster_occurrence'] = np.ones(dis_data['disaster_occurrence'].shape)\n",
    "# dis_data = dis_data.reset_index()\n",
    "\n",
    "# print(dis_data)\n",
    "\n",
    "dis_data.to_csv('./data/test_disasters_state_month.csv', index=False)\n",
    "\n",
    "# filter temperature dataset\n",
    "temp_data = pd.read_csv(temp_path)\n",
    "temp_data = temp_data[temp_data['Country'] == 'United States'].dropna()  # filter by United States, remove NaNs\n",
    "temp_data['date'] = temp_data['dt'].astype('datetime64[ns]').dt.strftime('%m-%Y')  # convert string to date, then convert to year\n",
    "temp_data['state'] = temp_data['State'].apply(lambda x: states[x] if x in states else None)  # preprocess state strings\n",
    "temp_data = temp_data.dropna()\n",
    "temp_data = temp_data.groupby(['date', 'state'])  # group by year then state\n",
    "temp_data = temp_data[['AverageTemperature', 'AverageTemperatureUncertainty']].mean().reset_index()  # take average over groups\n",
    "\n",
    "# print(temp_data)\n",
    "\n",
    "temp_data.to_csv('./data/test_temp_state_month.csv', index=False)\n",
    "\n",
    "# join on `Year` and `State`\n",
    "df = pd.merge(temp_data, dis_data, on=['date', 'state'], how='left').set_index(['date', 'state'], drop=True)\n",
    "df.rename({'AverageTemperature': 'ave_temp', 'AverageTemperatureUncertainty': 'ave_temp_uncertainty'}, axis=1, inplace=True)\n",
    "df = df.fillna(0).reset_index()\n",
    "df['month'] = df['date'].astype('datetime64[ns]').dt.strftime('%m')\n",
    "df['year'] = df['date'].astype('datetime64[ns]').dt.strftime('%Y')\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('./data/test_disasters_temp_state_month.csv', index=False)\n",
    "\n",
    "df['y_data'] = df['disaster_occurrence']\n",
    "df = df.drop(['disaster_occurrence'], axis=1)\n",
    "\n",
    "\n",
    "df = df[df.year > \"1960\"]\n",
    "df = df.drop(['year'], axis=1)\n",
    "\n",
    "print(df)\n",
    "\n",
    "x_data = df.iloc[:,0:-1]\n",
    "y_data = df.iloc[:,-1]\n",
    "\n",
    "x_data = x_data.drop(['declaration_title','disaster_type'], axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=.5, shuffle=False)\n",
    "\n",
    "train_rows = int(0.6 * x_data.shape[0])\n",
    "test_rows = x_data.shape[0] - train_rows# - train_rows\n",
    "\n",
    "x_train = x_data.iloc[:train_rows, 0:-1]\n",
    "y_train = x_data.iloc[:train_rows, -1]\n",
    "x_test = x_data.iloc[train_rows:, 0:-1]\n",
    "y_test = x_data.iloc[train_rows:, -1]\n",
    "    \n",
    "print(x_train)\n",
    "\n",
    "X = x_data.iloc[:,2:]\n",
    "y = y_data\n",
    "\n",
    "logR = LogisticRegression()\n",
    "logR.fit(x_train.iloc[:,2:], y_train)\n",
    "y_predict_test = logR.predict_proba(x_test.iloc[:,2:])\n",
    "y_predict_train = logR.predict_proba(x_train.iloc[:,2:])\n",
    "\n",
    "x_test['Disaster_Prob'] = y_predict_test[:,1:]\n",
    "\n",
    "\n",
    "y_predict_test = y_predict_test[:,1:]\n",
    "test_df = pd.DataFrame(y_predict_test, columns = ['Y_predict'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LogisticRegression_Test_Per = x_test[['date', 'state', 'Disaster_Prob']]\n",
    "#print(LogisticRegression_Test_Per.tail(50))\n",
    "\n",
    "LogisticRegression_Test_Per.to_csv('./data/logRPredictTest.csv', index=False)    \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "701125a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95494471 0.04505529]\n",
      " [0.84495342 0.15504658]\n",
      " [0.82706004 0.17293996]\n",
      " ...\n",
      " [0.907349   0.092651  ]\n",
      " [0.95457925 0.04542075]\n",
      " [0.82788755 0.17211245]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "def split_data(X, y, test_size = 0.2):       \n",
    "    import sklearn as sk    \n",
    "    return sk.model_selection.train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(X, y, test_size = 0.2)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB    \n",
    "GNBclf = GaussianNB()\n",
    "classfier = GNBclf.fit(X_train, y_train)\n",
    "\n",
    "def score_and_predict(classifier, X_train, y_train, X_test, y_test):    \n",
    "    y_Train_pred = classifier.predict_proba(X_train)\n",
    "    y_Test_pred = classifier.predict_proba(X_test)\n",
    "    #testing_score = metrics.accuracy_score(y_test, y_Test_pred)\n",
    "    #training_score = metrics.accuracy_score(y_train, y_Train_pred)\n",
    "    return y_Train_pred\n",
    "\n",
    "y_pred = score_and_predict(classfier, X_train, y_train, X_test, y_test)\n",
    "#print('Mean accuracy on training data = {} and testing data = {}'.format(training_score.round(2), testing_score.round(2)))\n",
    "print(y_pred)\n",
    "def print_classification_report(y_test, y_pred):       \n",
    "    from sklearn.metrics import classification_report\n",
    "    return (print(classification_report(y_test,y_pred)))\n",
    "\n",
    "#print_classification_report(y_test, y_pred)\n",
    "\n",
    "def roc_curve(classifier, X_test, y_test):      \n",
    "    from sklearn.metrics import roc_curve\n",
    "    import matplotlib.pyplot as plt       \n",
    "    predict_prob = classifier.predict_proba(X_test)\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test,predict_prob[:, 1])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')        \n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')   \n",
    "    return (plt.show())\n",
    "\n",
    "#roc_curve(classifier, X_test, y_test)\n",
    "from sklearn.svm import SVC\n",
    "SVMclf = SVC(probability=True) \n",
    "SVMclf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-species",
   "metadata": {},
   "source": [
    "# 1. Setting up a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "with open('data_test.csv', 'r') as file:\n",
    "    df = csv.reader(file)\n",
    "    for row in df:\n",
    "        print(row)\n",
    "        \n",
    "df = pd.read_csv('data_test.csv')\n",
    "        \n",
    "#Create the confusion matrix\n",
    "# Importing the dependancies\n",
    "\n",
    "y_pred = df[df.columns[-1]]\n",
    "# Actual values\n",
    "y_act = df[df.columns[2]]\n",
    "\n",
    "# Printing the confusion matrix\n",
    "# The columns will show the instances predicted for each label,\n",
    "# and the rows will show the actual number of instances for each label.\n",
    "\n",
    "print(y_pred)\n",
    "# Printing the precision and recall, among other metrics\n",
    "\n",
    "print(y_act)\n",
    "#confusion matrix\n",
    "print(metrics.confusion_matrix(y_act, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-winning",
   "metadata": {},
   "source": [
    "# 2. Finding other useful model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other model metrics\n",
    "print(metrics.classification_report(y_act, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-ferry",
   "metadata": {},
   "source": [
    "# 3. Create an ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Computing the ROC curve\n",
    "n_classes = y_act.shape[0]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_act[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_act.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#Plotting the ROC curve\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
