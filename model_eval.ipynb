{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8820e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f16ee8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c7b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8db057e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "99ea2b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import gc\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB    \n",
    "\n",
    "\n",
    "dis_path = './data/us_disaster_declarations.csv'\n",
    "temp_path = './data/GlobalLandTemperaturesByState.csv'\n",
    "states_path = './data/states.csv'\n",
    "\n",
    "# load list of states\n",
    "states = {}\n",
    "with open(states_path) as f:\n",
    "    next(f)\n",
    "\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        states[l[0].strip()] = l[1].strip()\n",
    "\n",
    "# print(states)\n",
    "\n",
    "# filter disaster dataset\n",
    "dis_data = pd.read_csv(dis_path)[['state', 'declaration_date', 'incident_type','declaration_title']].rename({'declaration_date': 'date'}, axis=1)\n",
    "dis_data['date'] = dis_data['date'].astype('datetime64[ns]').dt.strftime('%m-%Y')\n",
    "dis_data = dis_data.drop_duplicates(subset=['incident_type', 'declaration_title', 'date', 'state'], keep='first')\n",
    "# dis_data = dis_data.groupby(['state', 'date']).count()\n",
    "dis_data['disaster_type'] = dis_data['incident_type']\n",
    "dis_data = dis_data.rename({'incident_type': 'disaster_occurrence'}, axis=1)\n",
    "dis_data['disaster_occurrence'] = np.ones(dis_data['disaster_occurrence'].shape)\n",
    "# dis_data = dis_data.reset_index()\n",
    "\n",
    "# print(dis_data)\n",
    "\n",
    "dis_data.to_csv('./data/test_disasters_state_month.csv', index=False)\n",
    "\n",
    "# filter temperature dataset\n",
    "temp_data = pd.read_csv(temp_path)\n",
    "temp_data = temp_data[temp_data['Country'] == 'United States'].dropna()  # filter by United States, remove NaNs\n",
    "temp_data['date'] = temp_data['dt'].astype('datetime64[ns]').dt.strftime('%m-%Y')  # convert string to date, then convert to year\n",
    "temp_data['state'] = temp_data['State'].apply(lambda x: states[x] if x in states else None)  # preprocess state strings\n",
    "temp_data = temp_data.dropna()\n",
    "temp_data = temp_data.groupby(['date', 'state'])  # group by year then state\n",
    "temp_data = temp_data[['AverageTemperature', 'AverageTemperatureUncertainty']].mean().reset_index()  # take average over groups\n",
    "\n",
    "# print(temp_data)\n",
    "\n",
    "temp_data.to_csv('./data/test_temp_state_month.csv', index=False)\n",
    "\n",
    "# join on `Year` and `State`\n",
    "df = pd.merge(temp_data, dis_data, on=['date', 'state'], how='left').set_index(['date', 'state'], drop=True)\n",
    "df.rename({'AverageTemperature': 'ave_temp', 'AverageTemperatureUncertainty': 'ave_temp_uncertainty'}, axis=1, inplace=True)\n",
    "df = df.fillna(0).reset_index()\n",
    "df['month'] = df['date'].astype('datetime64[ns]').dt.strftime('%m')\n",
    "df['year'] = df['date'].astype('datetime64[ns]').dt.strftime('%Y')\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "\n",
    "df.to_csv('./data/test_disasters_temp_state_month.csv', index=False)\n",
    "\n",
    "df['y_data'] = df['disaster_occurrence']\n",
    "df = df.drop(['disaster_occurrence'], axis=1)\n",
    "\n",
    "\n",
    "df = df[df.year > \"1960\"]\n",
    "df = df.drop(['year'], axis=1)\n",
    "\n",
    "df = df.sort_values(by=['date'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3873d32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "e7498ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data):  \n",
    "    x_data = data.iloc[:,0:-1]\n",
    "    y_data = data.iloc[:,-1]\n",
    "    \n",
    "    train_rows = int(0.6 * x_data.shape[0])\n",
    "    test_rows = x_data.shape[0] - train_rows# - train_rows\n",
    "\n",
    "    x_train = data.iloc[:train_rows, 0:-1]\n",
    "    y_train = data.iloc[:train_rows, -1]\n",
    "    x_test = data.iloc[train_rows:, 0:-1]\n",
    "    y_test = data.iloc[train_rows:, -1]\n",
    "\n",
    "    X = x_data.iloc[:,2:]\n",
    "    y = y_data\n",
    "\n",
    "    x_test_plot = x_test.iloc[:,2:]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "f8e3e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logReg(x_train, y_train, x_test, y_test, disaster_type):\n",
    "    logR = LogisticRegression()\n",
    "    logR.fit(x_train.iloc[:,2:], y_train)\n",
    "    y_predict_test = logR.predict_proba(x_test.iloc[:,2:])\n",
    "    y_predict_train = logR.predict_proba(x_train.iloc[:,2:])\n",
    "\n",
    "    y_predict_test = y_predict_test[:,1:] \n",
    "\n",
    "    x_test['Disaster_Prob'] = y_predict_test\n",
    "    x_test['y_data'] = y_data\n",
    "\n",
    "    test_df = pd.DataFrame(y_predict_test, columns = ['Y_predict'])\n",
    "\n",
    "    test_actual = df.iloc[test_rows:,:]\n",
    "\n",
    "    test_actual=test_actual[['date', 'state', 'y_data']]\n",
    "    LogisticRegression_Test_Per = x_test[['date', 'state', 'Disaster_Prob']]\n",
    "    logResgressionPerTestCombined = x_test[['date', 'state', 'Disaster_Prob', 'y_data']]\n",
    "\n",
    "\n",
    "    #logResgressionPerTestCombined.to_csv('./data/logResgressionPerTestCombined_' + disaster_type + '.csv', index=False)                        \n",
    "    #LogisticRegression_Test_Per.to_csv('./data/logRPredictTest.csv', index=False)  \n",
    "    \n",
    "    roc = []\n",
    "\n",
    "    threshold = 0\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    for i in range(0,25):\n",
    "        y_predict_test_no_prob = np.where(y_predict_test >= threshold, 1, 0)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_predict_test_no_prob).ravel()\n",
    "        fpr = fp / (fp + tn)\n",
    "        tpr = tp / (tp + fn)\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "\n",
    "        roc.append([threshold, tn, fp, fn, tp, fpr, tpr, precision, recall])\n",
    "        threshold = threshold + .005\n",
    "\n",
    "    roc_df = pd.DataFrame(roc, columns =['Threshold', 'tn', 'fp', 'fn', 'tp','fpr', 'tpr', 'precision', 'recall']) \n",
    "    #print(roc_df)\n",
    "    #roc_df.to_csv('./data/roc_df_' + disaster_type + '.csv', index=False)\n",
    "    return logResgressionPerTestCombined, roc_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "37ed1e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hurricane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire\n",
      "Severe Storm(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxic Substances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drought\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dam/Levee Break\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earthquake\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tornado\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severe Ice Storm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coastal Storm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fishing Losses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mud/Landslide\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Cause\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n",
      "C:\\Ana\\envs\\hw4q3try3\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in longlong_scalars\n"
     ]
    }
   ],
   "source": [
    "disaster_list = df.disaster_type.unique()\n",
    "disaster_list[0] = 'All'\n",
    "exclued_list = ['Volcano', 'Typhoon', 'Terrorist', 'Tsunami']\n",
    "\n",
    "\n",
    "for disaster in disaster_list:\n",
    "    if disaster not in exclued_list:\n",
    "        print(disaster)\n",
    "        if disaster != \"All\":\n",
    "            df_model= df[(df.disaster_type == disaster) | (df.disaster_type == 0)]\n",
    "            df_model= df_model.drop(['declaration_title','disaster_type'], axis=1)\n",
    "            x_train, y_train, x_test, y_test = split_data(df_model)\n",
    "            append_logReg, append_roc = logReg(x_train, y_train, x_test, y_test, disaster)\n",
    "\n",
    "            append_logReg = append_logReg.assign(disaster_type=disaster)\n",
    "            append_roc = append_roc.assign(disaster_type=disaster)\n",
    "\n",
    "            combine_logR = [append_logReg, logReg_by_type]\n",
    "            logReg_by_type = pd.concat(combine_logR)\n",
    "\n",
    "            combine_roc = [append_roc, roc_df_by_type]\n",
    "            roc_df_by_type = pd.concat(combine_roc)\n",
    "        else:\n",
    "            df_model= df.drop(['declaration_title','disaster_type'], axis=1)\n",
    "            x_train, y_train, x_test, y_test = split_data(df_model)\n",
    "            logReg_by_type, roc_df_by_type = logReg(x_train, y_train, x_test, y_test, disaster)\n",
    "\n",
    "            logReg_by_type = logReg_by_type.assign(disaster_type=disaster)\n",
    "            roc_df_by_type = roc_df_by_type.assign(disaster_type=disaster)\n",
    "\n",
    "roc_df_by_type.to_csv('./data/roc_df_by_types.csv', index=False)\n",
    "logReg_by_type.to_csv('./data/logRPredictTest_by_types.csv', index=False) \n",
    "\n",
    "df_model= df.drop(['declaration_title','disaster_type'], axis=1)\n",
    "x_train, y_train, x_test, y_test = split_data(df_model)\n",
    "logReg, roc_df = logReg(x_train, y_train, x_test, y_test, disaster)\n",
    "\n",
    "roc_df.to_csv('./data/roc_df.csv', index=False)\n",
    "logReg.to_csv('./data/logReg.csv', index=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "11c2b324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Threshold     tn     fp    fn    tp       fpr       tpr\n",
      "0       0.000      0  10450     0  2237  1.000000  1.000000\n",
      "1       0.005     76  10374    11  2226  0.992727  0.995083\n",
      "2       0.010    176  10274    25  2212  0.983158  0.988824\n",
      "3       0.015    469   9981    45  2192  0.955120  0.979884\n",
      "4       0.020    914   9536    77  2160  0.912536  0.965579\n",
      "5       0.025   1480   8970   129  2108  0.858373  0.942333\n",
      "6       0.030   2111   8339   183  2054  0.797990  0.918194\n",
      "7       0.035   2700   7750   241  1996  0.741627  0.892266\n",
      "8       0.040   3282   7168   301  1936  0.685933  0.865445\n",
      "9       0.045   3902   6548   386  1851  0.626603  0.827447\n",
      "10      0.050   4489   5961   479  1758  0.570431  0.785874\n",
      "11      0.055   5062   5388   567  1670  0.515598  0.746536\n",
      "12      0.060   5589   4861   747  1490  0.465167  0.666071\n",
      "13      0.065   6233   4217   883  1354  0.403541  0.605275\n",
      "14      0.070   6979   3471  1046  1191  0.332153  0.532409\n",
      "15      0.075   7862   2588  1329   908  0.247656  0.405901\n",
      "16      0.080   8795   1655  1638   599  0.158373  0.267769\n",
      "17      0.085   9738    712  2016   221  0.068134  0.098793\n",
      "18      0.090  10449      1  2230     7  0.000096  0.003129\n",
      "19      0.095  10450      0  2237     0  0.000000  0.000000\n",
      "20      0.100  10450      0  2237     0  0.000000  0.000000\n",
      "21      0.105  10450      0  2237     0  0.000000  0.000000\n",
      "22      0.110  10450      0  2237     0  0.000000  0.000000\n",
      "23      0.115  10450      0  2237     0  0.000000  0.000000\n",
      "24      0.120  10450      0  2237     0  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "GNBclf = GaussianNB()\n",
    "GNBclf.fit(x_train.iloc[:,2:], y_train)\n",
    "y_predict_test_GNB = GNBclf.predict_proba(x_test_plot)\n",
    "y_predict_train_GNB = GNBclf.predict_proba(x_train.iloc[:,2:])\n",
    "\n",
    "y_predict_test_GNB = y_predict_test_GNB[:,1:]\n",
    "y_predict_test_GNB = np.round(y_predict_test_GNB, 5)\n",
    "test_df_GNB = pd.DataFrame(y_predict_test_GNB, columns = ['Y_predict'])\n",
    "test_df_GNB.to_csv('./data/test_df_GNB.csv', index=False)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_roc_curve\n",
    "\n",
    "roc_GNB = []\n",
    "\n",
    "threshold = 0\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "for i in range(0,25):\n",
    "    y_predict_test_no_prob = np.where(y_predict_test_GNB >= threshold, 1, 0)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_predict_test_no_prob).ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "    tpr = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "    roc_GNB.append([threshold, tn, fp, fn, tp, fpr, tpr])\n",
    "    threshold = threshold + .005\n",
    "\n",
    "roc_GNB_df = pd.DataFrame(roc_GNB, columns =['Threshold', 'tn', 'fp', 'fn', 'tp','fpr', 'tpr']) \n",
    "print(roc_GNB_df)\n",
    "roc_GNB_df.to_csv('./data/prc_GNB_df.csv', index=False) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3fb48503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Threshold     tn     fp    fn    tp       fpr       tpr\n",
      "0       0.000      0  10450     0  2237  1.000000  1.000000\n",
      "1       0.005      0  10450     0  2237  1.000000  1.000000\n",
      "2       0.010      0  10450     0  2237  1.000000  1.000000\n",
      "3       0.015      0  10450     0  2237  1.000000  1.000000\n",
      "4       0.020      0  10450     0  2237  1.000000  1.000000\n",
      "5       0.025      0  10450     0  2237  1.000000  1.000000\n",
      "6       0.030      0  10450     0  2237  1.000000  1.000000\n",
      "7       0.035      0  10450     0  2237  1.000000  1.000000\n",
      "8       0.040      0  10450     0  2237  1.000000  1.000000\n",
      "9       0.045      0  10450     0  2237  1.000000  1.000000\n",
      "10      0.050    235  10215    40  2197  0.977512  0.982119\n",
      "11      0.055   8157   2293  1838   399  0.219426  0.178364\n",
      "12      0.060   9981    469  2131   106  0.044880  0.047385\n",
      "13      0.065  10447      3  2194    43  0.000287  0.019222\n",
      "14      0.070  10449      1  2221    16  0.000096  0.007152\n",
      "15      0.075  10449      1  2234     3  0.000096  0.001341\n",
      "16      0.080  10449      1  2234     3  0.000096  0.001341\n",
      "17      0.085  10449      1  2237     0  0.000096  0.000000\n",
      "18      0.090  10449      1  2237     0  0.000096  0.000000\n",
      "19      0.095  10449      1  2237     0  0.000096  0.000000\n",
      "20      0.100  10449      1  2237     0  0.000096  0.000000\n",
      "21      0.105  10450      0  2237     0  0.000000  0.000000\n",
      "22      0.110  10450      0  2237     0  0.000000  0.000000\n",
      "23      0.115  10450      0  2237     0  0.000000  0.000000\n",
      "24      0.120  10450      0  2237     0  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "svc_clf = SVC(probability=True)\n",
    "svc_clf.fit(x_train.iloc[:,2:], y_train)\n",
    "y_predict_test_SVC = svc_clf.predict_proba(x_test_plot)\n",
    "\n",
    "y_predict_test_SVC = y_predict_test_SVC[:,1:]\n",
    "\n",
    "\n",
    "test_df_SVC = pd.DataFrame(y_predict_test_SVC, columns = ['Y_predict'])\n",
    "test_df_SVC.to_csv('./data/test_df_SVC.csv', index=False)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_roc_curve\n",
    "\n",
    "roc_SVC = []\n",
    "\n",
    "threshold = 0\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "for i in range(0,25):\n",
    "    y_predict_test_no_prob = np.where(y_predict_test_SVC >= threshold, 1, 0)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_predict_test_no_prob).ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "    tpr = tp / (tp + fn)\n",
    "    \n",
    "    roc_SVC.append([threshold, tn, fp, fn, tp, fpr, tpr])\n",
    "    threshold = threshold + .005\n",
    "\n",
    "roc_SVC_df = pd.DataFrame(roc_SVC, columns =['Threshold', 'tn', 'fp', 'fn', 'tp','fpr', 'tpr']) \n",
    "print(roc_SVC_df)\n",
    "roc_SVC_df.to_csv('./data/prc_SVC_df.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69642d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
