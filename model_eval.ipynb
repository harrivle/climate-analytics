{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd46a0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d42cf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd34d140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1db44f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import gc\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB    \n",
    "\n",
    "\n",
    "dis_path = './data/us_disaster_declarations.csv'\n",
    "temp_path = './data/GlobalLandTemperaturesByState.csv'\n",
    "states_path = './data/states.csv'\n",
    "\n",
    "# load list of states\n",
    "states = {}\n",
    "with open(states_path) as f:\n",
    "    next(f)\n",
    "\n",
    "    for line in f:\n",
    "        l = line.split(',')\n",
    "        states[l[0].strip()] = l[1].strip()\n",
    "\n",
    "# print(states)\n",
    "\n",
    "# filter disaster dataset\n",
    "dis_data = pd.read_csv(dis_path)[['state', 'declaration_date', 'incident_type','declaration_title']].rename({'declaration_date': 'date'}, axis=1)\n",
    "dis_data['date'] = dis_data['date'].astype('datetime64[ns]').dt.strftime('%m-%Y')\n",
    "dis_data = dis_data.drop_duplicates(subset=['incident_type', 'declaration_title', 'date', 'state'], keep='first')\n",
    "# dis_data = dis_data.groupby(['state', 'date']).count()\n",
    "dis_data['disaster_type'] = dis_data['incident_type']\n",
    "dis_data = dis_data.rename({'incident_type': 'disaster_occurrence'}, axis=1)\n",
    "dis_data['disaster_occurrence'] = np.ones(dis_data['disaster_occurrence'].shape)\n",
    "# dis_data = dis_data.reset_index()\n",
    "\n",
    "# print(dis_data)\n",
    "\n",
    "dis_data.to_csv('./data/test_disasters_state_month.csv', index=False)\n",
    "\n",
    "# filter temperature dataset\n",
    "temp_data = pd.read_csv(temp_path)\n",
    "temp_data = temp_data[temp_data['Country'] == 'United States'].dropna()  # filter by United States, remove NaNs\n",
    "temp_data['date'] = temp_data['dt'].astype('datetime64[ns]').dt.strftime('%m-%Y')  # convert string to date, then convert to year\n",
    "temp_data['state'] = temp_data['State'].apply(lambda x: states[x] if x in states else None)  # preprocess state strings\n",
    "temp_data = temp_data.dropna()\n",
    "temp_data = temp_data.groupby(['date', 'state'])  # group by year then state\n",
    "temp_data = temp_data[['AverageTemperature', 'AverageTemperatureUncertainty']].mean().reset_index()  # take average over groups\n",
    "\n",
    "# print(temp_data)\n",
    "\n",
    "temp_data.to_csv('./data/test_temp_state_month.csv', index=False)\n",
    "\n",
    "# join on `Year` and `State`\n",
    "df = pd.merge(temp_data, dis_data, on=['date', 'state'], how='left').set_index(['date', 'state'], drop=True)\n",
    "df.rename({'AverageTemperature': 'ave_temp', 'AverageTemperatureUncertainty': 'ave_temp_uncertainty'}, axis=1, inplace=True)\n",
    "df = df.fillna(0).reset_index()\n",
    "df['month'] = df['date'].astype('datetime64[ns]').dt.strftime('%m')\n",
    "df['year'] = df['date'].astype('datetime64[ns]').dt.strftime('%Y')\n",
    "df['date'] = pd.to_datetime(df.date)\n",
    "\n",
    "\n",
    "df.to_csv('./data/test_disasters_temp_state_month.csv', index=False)\n",
    "\n",
    "df['y_data'] = df['disaster_occurrence']\n",
    "df = df.drop(['disaster_occurrence'], axis=1)\n",
    "\n",
    "\n",
    "df = df[df.year > \"1960\"]\n",
    "df = df.drop(['year'], axis=1)\n",
    "\n",
    "df = df.sort_values(by=['date'])\n",
    "x_data = df.iloc[:,0:-1]\n",
    "y_data = df.iloc[:,-1]\n",
    "\n",
    "df = df.drop(['declaration_title','disaster_type'], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3aa8ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=.5, shuffle=False)\n",
    "\n",
    "train_rows = int(0.6 * x_data.shape[0])\n",
    "test_rows = x_data.shape[0] - train_rows# - train_rows\n",
    "\n",
    "x_train = df.iloc[:train_rows, 0:-1]\n",
    "y_train = df.iloc[:train_rows, -1]\n",
    "x_test = df.iloc[train_rows:, 0:-1]\n",
    "y_test = df.iloc[train_rows:, -1]\n",
    "    \n",
    "X = x_data.iloc[:,2:]\n",
    "y = y_data\n",
    "\n",
    "x_test_plot = x_test.iloc[:,2:]\n",
    "\n",
    "logR = LogisticRegression()\n",
    "logR.fit(x_train.iloc[:,2:], y_train)\n",
    "y_predict_test = logR.predict_proba(x_test.iloc[:,2:])\n",
    "y_predict_train = logR.predict_proba(x_train.iloc[:,2:])\n",
    "\n",
    "y_predict_test = y_predict_test[:,1:]\n",
    "\n",
    "  \n",
    "x_test['Disaster_Prob'] = y_predict_test\n",
    "x_test['y_data'] = y_data\n",
    "\n",
    "test_df = pd.DataFrame(y_predict_test, columns = ['Y_predict'])\n",
    "\n",
    "test_actual = df.iloc[test_rows:,:]\n",
    "\n",
    "test_actual=test_actual[['date', 'state', 'y_data']]\n",
    "LogisticRegression_Test_Per = x_test[['date', 'state', 'Disaster_Prob']]\n",
    "logResgressionPerTestCombined = x_test[['date', 'state', 'Disaster_Prob', 'y_data']]\n",
    "\n",
    "\n",
    "logResgressionPerTestCombined.to_csv('./data/logResgressionPerTestCombined.csv', index=False)                        \n",
    "test_actual.to_csv('./data/test_actual.csv', index=False)\n",
    "LogisticRegression_Test_Per.to_csv('./data/logRPredictTest.csv', index=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "13276e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Threshold     tn     fp    fn    tp       fpr       tpr\n",
      "0       0.000      0  10450     0  2237  1.000000  1.000000\n",
      "1       0.005      0  10450     0  2237  1.000000  1.000000\n",
      "2       0.010      5  10445     0  2237  0.999522  1.000000\n",
      "3       0.015     46  10404     7  2230  0.995598  0.996871\n",
      "4       0.020    287  10163    19  2218  0.972536  0.991506\n",
      "5       0.025    773   9677    54  2183  0.926029  0.975861\n",
      "6       0.030   1403   9047   101  2136  0.865742  0.954850\n",
      "7       0.035   2145   8305   169  2068  0.794737  0.924452\n",
      "8       0.040   3008   7442   259  1978  0.712153  0.884220\n",
      "9       0.045   3893   6557   358  1879  0.627464  0.839964\n",
      "10      0.050   4862   5588   519  1718  0.534737  0.767993\n",
      "11      0.055   5750   4700   653  1584  0.449761  0.708091\n",
      "12      0.060   6690   3760   864  1373  0.359809  0.613768\n",
      "13      0.065   7604   2846  1141  1096  0.272344  0.489942\n",
      "14      0.070   8376   2074  1447   790  0.198469  0.353152\n",
      "15      0.075   9032   1418  1690   547  0.135694  0.244524\n",
      "16      0.080   9579    871  1879   358  0.083349  0.160036\n",
      "17      0.085   9961    489  2002   235  0.046794  0.105051\n",
      "18      0.090  10255    195  2125   112  0.018660  0.050067\n",
      "19      0.095  10405     45  2178    59  0.004306  0.026375\n",
      "20      0.100  10443      7  2216    21  0.000670  0.009388\n",
      "21      0.105  10450      0  2237     0  0.000000  0.000000\n",
      "22      0.110  10450      0  2237     0  0.000000  0.000000\n",
      "23      0.115  10450      0  2237     0  0.000000  0.000000\n",
      "24      0.120  10450      0  2237     0  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "prc = []\n",
    "\n",
    "threshold = 0\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "for i in range(0,25):\n",
    "    y_predict_test_no_prob = np.where(y_predict_test >= threshold, 1, 0)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_predict_test_no_prob).ravel()\n",
    "    fpr = fp / (fp + tn)\n",
    "    tpr = tp / (tp + fn)\n",
    "    \n",
    "    prc.append([threshold, tn, fp, fn, tp, fpr, tpr])\n",
    "    threshold = threshold + .005\n",
    "\n",
    "prc_df = pd.DataFrame(prc, columns =['Threshold', 'tn', 'fp', 'fn', 'tp','fpr', 'tpr']) \n",
    "print(prc_df)\n",
    "prc_df.to_csv('./data/prc_df.csv', index=False) \n",
    "\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_predict_test_no_prob)\n",
    "test_avg_pers = average_precision_score(y_test, y_predict_test_no_prob)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a10875e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.95494471 0.04505529]\n",
      " [0.84495342 0.15504658]\n",
      " [0.82706004 0.17293996]\n",
      " ...\n",
      " [0.907349   0.092651  ]\n",
      " [0.95457925 0.04542075]\n",
      " [0.82788755 0.17211245]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "GNBclf = GaussianNB()\n",
    "classfier = GNBclf.fit(x_train.iloc[:,2:], y_train)\n",
    "y_Train_pred_Gaus = GNBclf.predict_proba(x_train.iloc[:,2:])\n",
    "y_predict_test_Gaus = GNBclf.predict_proba(x_test.iloc[:,2:])\n",
    "\n",
    "y_predict_test_Gaus = y_predict_test_Gaus[:,1:]\n",
    "print(y_predict_test_Gaus)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-species",
   "metadata": {},
   "source": [
    "# 1. Setting up a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "with open('data_test.csv', 'r') as file:\n",
    "    df = csv.reader(file)\n",
    "    for row in df:\n",
    "        print(row)\n",
    "        \n",
    "df = pd.read_csv('data_test.csv')\n",
    "        \n",
    "#Create the confusion matrix\n",
    "# Importing the dependancies\n",
    "\n",
    "y_pred = df[df.columns[-1]]\n",
    "# Actual values\n",
    "y_act = df[df.columns[2]]\n",
    "\n",
    "# Printing the confusion matrix\n",
    "# The columns will show the instances predicted for each label,\n",
    "# and the rows will show the actual number of instances for each label.\n",
    "\n",
    "print(y_pred)\n",
    "# Printing the precision and recall, among other metrics\n",
    "\n",
    "print(y_act)\n",
    "#confusion matrix\n",
    "print(metrics.confusion_matrix(y_act, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-winning",
   "metadata": {},
   "source": [
    "# 2. Finding other useful model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other model metrics\n",
    "print(metrics.classification_report(y_act, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-ferry",
   "metadata": {},
   "source": [
    "# 3. Create an ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Computing the ROC curve\n",
    "n_classes = y_act.shape[0]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_act[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_act.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#Plotting the ROC curve\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
