{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4b8a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a25a89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661eeb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2bb3b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date state  ave_temp  ave_temp_uncertainty declaration_title  \\\n",
      "0       01-1745    AL     6.931                 2.838                 0   \n",
      "1       01-1745    CT    -1.734                 1.543                 0   \n",
      "2       01-1745    DE     1.175                 1.921                 0   \n",
      "3       01-1745    FL    14.640                 2.447                 0   \n",
      "4       01-1745    IA    -8.066                 3.766                 0   \n",
      "...         ...   ...       ...                   ...               ...   \n",
      "136388  12-2012    VT    -2.191                 0.343                 0   \n",
      "136389  12-2012    WA     0.290                 0.307                 0   \n",
      "136390  12-2012    WI    -3.367                 0.301                 0   \n",
      "136391  12-2012    WV     4.280                 0.240                 0   \n",
      "136392  12-2012    WY    -5.374                 0.375                 0   \n",
      "\n",
      "       disaster_type month  y_data  \n",
      "0                  0    01     0.0  \n",
      "1                  0    01     0.0  \n",
      "2                  0    01     0.0  \n",
      "3                  0    01     0.0  \n",
      "4                  0    01     0.0  \n",
      "...              ...   ...     ...  \n",
      "136388             0    12     0.0  \n",
      "136389             0    12     0.0  \n",
      "136390             0    12     0.0  \n",
      "136391             0    12     0.0  \n",
      "136392             0    12     0.0  \n",
      "\n",
      "[136393 rows x 8 columns]\n",
      "           date state  ave_temp  ave_temp_uncertainty declaration_title  \\\n",
      "0       01-1745    AL     6.931                 2.838                 0   \n",
      "1       01-1745    CT    -1.734                 1.543                 0   \n",
      "2       01-1745    DE     1.175                 1.921                 0   \n",
      "3       01-1745    FL    14.640                 2.447                 0   \n",
      "4       01-1745    IA    -8.066                 3.766                 0   \n",
      "...         ...   ...       ...                   ...               ...   \n",
      "136388  12-2012    VT    -2.191                 0.343                 0   \n",
      "136389  12-2012    WA     0.290                 0.307                 0   \n",
      "136390  12-2012    WI    -3.367                 0.301                 0   \n",
      "136391  12-2012    WV     4.280                 0.240                 0   \n",
      "136392  12-2012    WY    -5.374                 0.375                 0   \n",
      "\n",
      "       disaster_type month  \n",
      "0                  0    01  \n",
      "1                  0    01  \n",
      "2                  0    01  \n",
      "3                  0    01  \n",
      "4                  0    01  \n",
      "...              ...   ...  \n",
      "136388             0    12  \n",
      "136389             0    12  \n",
      "136390             0    12  \n",
      "136391             0    12  \n",
      "136392             0    12  \n",
      "\n",
      "[136393 rows x 7 columns]\n",
      "0         0.0\n",
      "1         0.0\n",
      "2         0.0\n",
      "3         0.0\n",
      "4         0.0\n",
      "         ... \n",
      "136388    0.0\n",
      "136389    0.0\n",
      "136390    0.0\n",
      "136391    0.0\n",
      "136392    0.0\n",
      "Name: y_data, Length: 136393, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import gc\n",
    "import random\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def main():\n",
    "    dis_path = './data/us_disaster_declarations.csv'\n",
    "    temp_path = './data/GlobalLandTemperaturesByState.csv'\n",
    "    states_path = './data/states.csv'\n",
    "\n",
    "    # load list of states\n",
    "    states = {}\n",
    "    with open(states_path) as f:\n",
    "        next(f)\n",
    "\n",
    "        for line in f:\n",
    "            l = line.split(',')\n",
    "            states[l[0].strip()] = l[1].strip()\n",
    "\n",
    "    # print(states)\n",
    "\n",
    "    # filter disaster dataset\n",
    "    dis_data = pd.read_csv(dis_path)[['state', 'declaration_date', 'incident_type','declaration_title']].rename({'declaration_date': 'date'}, axis=1)\n",
    "    dis_data['date'] = dis_data['date'].astype('datetime64[ns]').dt.strftime('%m-%Y')\n",
    "    dis_data = dis_data.drop_duplicates(subset=['incident_type', 'declaration_title', 'date', 'state'], keep='first')\n",
    "    # dis_data = dis_data.groupby(['state', 'date']).count()\n",
    "    dis_data['disaster_type'] = dis_data['incident_type']\n",
    "    dis_data = dis_data.rename({'incident_type': 'disaster_occurrence'}, axis=1)\n",
    "    dis_data['disaster_occurrence'] = np.ones(dis_data['disaster_occurrence'].shape)\n",
    "    # dis_data = dis_data.reset_index()\n",
    "\n",
    "    # print(dis_data)\n",
    "\n",
    "    dis_data.to_csv('./data/test_disasters_state_month.csv', index=False)\n",
    "\n",
    "    # filter temperature dataset\n",
    "    temp_data = pd.read_csv(temp_path)\n",
    "    temp_data = temp_data[temp_data['Country'] == 'United States'].dropna()  # filter by United States, remove NaNs\n",
    "    temp_data['date'] = temp_data['dt'].astype('datetime64[ns]').dt.strftime('%m-%Y')  # convert string to date, then convert to year\n",
    "    temp_data['state'] = temp_data['State'].apply(lambda x: states[x] if x in states else None)  # preprocess state strings\n",
    "    temp_data = temp_data.dropna()\n",
    "    temp_data = temp_data.groupby(['date', 'state'])  # group by year then state\n",
    "    temp_data = temp_data[['AverageTemperature', 'AverageTemperatureUncertainty']].mean().reset_index()  # take average over groups\n",
    "\n",
    "    # print(temp_data)\n",
    "\n",
    "    temp_data.to_csv('./data/test_temp_state_month.csv', index=False)\n",
    "\n",
    "    # join on `Year` and `State`\n",
    "    df = pd.merge(temp_data, dis_data, on=['date', 'state'], how='left').set_index(['date', 'state'], drop=True)\n",
    "    df.rename({'AverageTemperature': 'ave_temp', 'AverageTemperatureUncertainty': 'ave_temp_uncertainty'}, axis=1, inplace=True)\n",
    "    df = df.fillna(0).reset_index()\n",
    "    df['month'] = df['date'].astype('datetime64[ns]').dt.strftime('%m')\n",
    "    \n",
    "   \n",
    "\n",
    "    df.to_csv('./data/test_disasters_temp_state_month.csv', index=False)\n",
    "    \n",
    "    df['y_data'] = df['disaster_occurrence']\n",
    "    df = df.drop(['disaster_occurrence'], axis=1)\n",
    "    print(df)\n",
    "    \n",
    "    x_data = df.iloc[:,0:-1]\n",
    "    y_data = df.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=.7, random_state=614, shuffle=True)\n",
    "    \n",
    "    print(x_data)\n",
    "    print(y_data)\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-species",
   "metadata": {},
   "source": [
    "# 1. Setting up a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cheap-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "with open('data_test.csv', 'r') as file:\n",
    "    df = csv.reader(file)\n",
    "    for row in df:\n",
    "        print(row)\n",
    "        \n",
    "df = pd.read_csv('data_test.csv')\n",
    "        \n",
    "#Create the confusion matrix\n",
    "# Importing the dependancies\n",
    "\n",
    "y_pred = df[df.columns[-1]]\n",
    "# Actual values\n",
    "y_act = df[df.columns[2]]\n",
    "\n",
    "# Printing the confusion matrix\n",
    "# The columns will show the instances predicted for each label,\n",
    "# and the rows will show the actual number of instances for each label.\n",
    "\n",
    "print(y_pred)\n",
    "# Printing the precision and recall, among other metrics\n",
    "\n",
    "print(y_act)\n",
    "#confusion matrix\n",
    "print(metrics.confusion_matrix(y_act, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-winning",
   "metadata": {},
   "source": [
    "# 2. Finding other useful model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other model metrics\n",
    "print(metrics.classification_report(y_act, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-ferry",
   "metadata": {},
   "source": [
    "# 3. Create an ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-honor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#Computing the ROC curve\n",
    "n_classes = y_act.shape[0]\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_act[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_act.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "#Plotting the ROC curve\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-clothing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
